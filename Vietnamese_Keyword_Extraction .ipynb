{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7208da",
   "metadata": {},
   "source": [
    "# Code KeyBert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a62594",
   "metadata": {},
   "source": [
    "## Related document:\n",
    "*  [Keyword Extraction Methods from Documents in NLP](https://www.analyticsvidhya.com/blog/2022/03/keyword-extraction-methods-from-documents-in-nlp/#:~:text=Keyword%20extraction%20is%20commonly%20used,and%20phrases%20from%20text%20input.)\n",
    "*  [py_vncorenlp library](https://github.com/vncorenlp/VnCoreNLP#install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ccc403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "    Chiến dịch quân sự đặc biệt vẫn tiếp tục và nó sẽ không dừng cho đến khi đạt được các mục tiêu đã đặt ra từ đầu\", đại diện Điện Kremlin khẳng định trong cuộc họp báo chiều 12-9.\n",
    "\n",
    "Khi được hỏi rằng liệu Tổng thống Putin có tin tưởng các tướng lĩnh của ông hay không, ông Peskov đã uyển chuyển đổi vấn đề.\n",
    "\n",
    "\"Bất kỳ hoạt động quân sự nào thuộc chiến dịch quân sự đặc biệt ở Ukraine đều được báo cáo cho tổng tư lệnh tối cao (tổng thống Nga - PV). Tổng thống liên lạc suốt ngày đêm với bộ trưởng quốc phòng và các tướng lĩnh\", ông Peskov nói.\n",
    "\n",
    "Hồi cuối tuần trước, Bộ Quốc phòng Nga xác nhận đang rút quân khỏi Kharkov để củng cố mục tiêu \"giải phóng Donbass\". Thông báo được đưa ra chỉ một ngày sau khi Nga tuyên bố đang điều động binh sĩ tăng viện cho Kharkov.\n",
    "\n",
    "Ukraine sau đó đẩy mạnh hoạt động tại mặt trận đông bắc, tuyên bố giành lại hơn 3.000km2 lãnh thổ chỉ sau 5 ngày và tiến vào kiểm soát toàn bộ tỉnh Kharkov vào ngày 11-9.\n",
    "\n",
    "Tại mặt trận phía nam, phía Ukraine cũng tuyên bố đã \"giải phóng\" được 500km2 trong hai tuần qua, tính từ khi mở đợt phản công.\n",
    "\n",
    "Các thông tin từ Ukraine khiến mọi sự chú ý đều đổ dồn về Nga, với nhiều ý kiến cho rằng Matxcơva đang thất thế trên chiến trường vì nhiều lý do.\n",
    "\n",
    "Theo Hãng tin Reuters, sự im lặng mấy ngày qua của Nga đã khiến một số nhà bình luận và những người theo chủ nghĩa dân tộc Nga tức giận.\n",
    "\n",
    "Cuộc họp báo chiều 12-9 là lần đầu tiên Điện Kremlin lên tiếng về chiến dịch phản công của Ukraine và hành động của Nga trong vài ngày qua.\n",
    "\n",
    "Khi được hỏi liệu Tổng thống Putin có ra lệnh tổng động viên để đáp trả cuộc phản công của Ukraine hay không, ông Peskov đã chuyển đề nghị báo chí hỏi Bộ Quốc phòng. Cho đến thời điểm hiện tại, ông Putin vẫn chưa huy động lực lượng dự bị khoảng 2 triệu binh sĩ.\n",
    "\n",
    "Ông Peskov cũng cho biết Nga không thấy có triển vọng về các cuộc đàm phán hòa bình với Ukraine nhưng không nói thêm chi tiết.\n",
    "\n",
    "Trong diễn biến khác liên quan cùng ngày 12-9, Bộ Quốc phòng Nga xác nhận các lực lượng không quân, tên lửa và pháo binh Nga đang \"thực hiện các cuộc tấn công chính xác vào các đơn vị và lực lượng dự bị của lực lượng vũ trang Ukraine\", bao gồm cả ở các trung tâm đô thị Kupiansk và Izium.\n",
    "\n",
    "Các cuộc tấn công cũng diễn ra ở mặt trận phía nam, tập trung vào thành phố Kherson. Theo ước tính của Nga, khoảng 300 binh sĩ Ukraine đã thiệt mạng và 1.000 binh sĩ khác bị thương vì các đợt tập kích của Nga trong 24 giờ qua.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f404f49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VnCoreNLP model folder C:/Users/duong/OneDrive/Desktop/VnCoreNLP-master already exists! Please load VnCoreNLP from this folder!\n"
     ]
    }
   ],
   "source": [
    "import py_vncorenlp\n",
    "# Automatically download VnCoreNLP components from the original repository\n",
    "# and save them in some local working folder\n",
    "py_vncorenlp.download_model(save_dir=\"C:/Users/duong/OneDrive/Desktop/VnCoreNLP-master\")\n",
    "\n",
    "# Load the word and sentence segmentation component\n",
    "rdrsegmenter  = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir=\"C:/Users/duong/OneDrive/Desktop/VnCoreNLP-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3925ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the new_doc using VnCoreNLP\n",
    "doc_segmented = rdrsegmenter.word_segment(doc)\n",
    "doc = ' '.join(doc_segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d691fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f40cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '11', '12', '24', '300', '500km2', 'bao_gồm', 'binh_sĩ',\n",
       "       'biết', 'báo_chí', 'báo_cáo', 'bình_luận', 'bất_kỳ', 'bắc',\n",
       "       'bị_thương', 'bộ', 'bộ_trưởng', 'chi_tiết', 'chiến_dịch',\n",
       "       'chiến_trường', 'chiều', 'cho', 'chuyển', 'chính_xác', 'chú_ý',\n",
       "       'chưa', 'chỉ', 'chủ_nghĩa', 'cuối', 'cuộc', 'các', 'có', 'cùng',\n",
       "       'cũng', 'cả', 'của', 'củng_cố', 'diễn', 'diễn_biến', 'donbass',\n",
       "       'dân_tộc', 'dừng', 'dự_bị', 'giành', 'giải_phóng', 'giờ', 'hai',\n",
       "       'hay', 'hiện_tại', 'hoà_bình', 'hoạt_động', 'huy_động',\n",
       "       'hành_động', 'hãng', 'hơn', 'họp_báo', 'hỏi', 'hồi', 'im_lặng',\n",
       "       'izium', 'kharkov', 'kherson', 'khi', 'khiến', 'khoảng', 'khác',\n",
       "       'không', 'không_quân', 'khẳng_định', 'khỏi', 'kiểm_soát', 'km2',\n",
       "       'kremlin', 'kupiansk', 'liên_lạc', 'liên_quan', 'liệu', 'là',\n",
       "       'lãnh_thổ', 'lên_tiếng', 'lý_do', 'lại', 'lần', 'lực_lượng',\n",
       "       'lực_lượng_vũ_trang', 'matxcơva', 'mấy', 'mặt_trận', 'mọi', 'một',\n",
       "       'một_số', 'mở', 'mục_tiêu', 'nam', 'nga', 'ngày', 'ngày_đêm',\n",
       "       'người', 'nhiều', 'nhà', 'nhưng', 'những', 'nào', 'nó', 'nói',\n",
       "       'peskov', 'pháo_binh', 'phía', 'phản_công', 'putin', 'pv', 'qua',\n",
       "       'quân', 'quân_sự', 'quốc_phòng', 'ra', 'ra_lệnh', 'reuters', 'rút',\n",
       "       'rằng', 'sau', 'suốt', 'sẽ', 'sự', 'theo', 'thiệt_mạng', 'thuộc',\n",
       "       'thành_phố', 'thêm', 'thông_báo', 'thông_tin', 'thất_thế', 'thấy',\n",
       "       'thời_điểm', 'thực_hiện', 'tin', 'tin_tưởng', 'tiến', 'tiếp_tục',\n",
       "       'toàn_bộ', 'triển_vọng', 'triệu', 'trong', 'trung_tâm', 'trên',\n",
       "       'trước', 'trả', 'tuyên_bố', 'tuần', 'tên_lửa', 'tính', 'tăng_viện',\n",
       "       'tướng_lĩnh', 'tại', 'tấn_công', 'tập_kích', 'tập_trung', 'tỉnh',\n",
       "       'tối_cao', 'tổng_thống', 'tổng_tư_lệnh', 'tổng_động_viên',\n",
       "       'tức_giận', 'từ', 'ukraine', 'uyển_chuyển', 'và', 'vài', 'vào',\n",
       "       'vì', 'vấn_đề', 'vẫn', 'về', 'với', 'xác_nhận', 'ông', 'ý_kiến',\n",
       "       'đang', 'điều_động', 'điện', 'đàm_phán', 'đáp', 'đã', 'đó',\n",
       "       'đô_thị', 'đông', 'đơn_vị', 'đưa', 'được', 'đại_diện', 'đạt',\n",
       "       'đầu', 'đầu_tiên', 'đẩy_mạnh', 'đặc_biệt', 'đặt', 'đến', 'đề_nghị',\n",
       "       'đều', 'để', 'đổ_dồn', 'đổi', 'đợt', 'ước_tính'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66424712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the document and candidates into embeddings using SentenceTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f79316f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8858213 , -0.15350103,  0.36514798, ..., -0.8275339 ,\n",
       "        -0.07458021, -0.1764793 ],\n",
       "       [-0.3906884 ,  0.08746338,  0.44342422, ..., -0.00321999,\n",
       "         0.20976323, -1.1869364 ],\n",
       "       [ 0.27796015, -0.27739087,  0.8311475 , ..., -0.57313395,\n",
       "        -0.15762645, -0.7089613 ],\n",
       "       ...,\n",
       "       [-0.7066388 ,  0.07808059,  0.3114909 , ..., -0.72212875,\n",
       "        -0.18518448,  0.25099245],\n",
       "       [-0.7639303 ,  0.19774228,  0.16978465, ..., -0.7230269 ,\n",
       "        -0.12861674,  0.34360802],\n",
       "       [-0.55979013,  0.32651833,  0.13383369, ..., -0.8262952 ,\n",
       "         0.20538029,  0.22841264]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe2d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Calculate cosine similarity between the document embedding and candidate embeddings\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "# Sort the indices of candidates based on their similarity to the document in ascending order\n",
    "# We only have one document, so distances.argsort()[0] gives the sorted indices\n",
    "sorted_indices = distances.argsort()[0]\n",
    "\n",
    "# Select the last 'top_n' elements from the sorted indices, which correspond to the top 'top_n' candidates\n",
    "top_n = 5  # Number of top candidates to select\n",
    "selected_indices = sorted_indices[-top_n:]\n",
    "\n",
    "# Create a list of the top 'top_n' candidates' names using the selected indices\n",
    "keywords = [candidates[index] for index in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5f1583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['đàm_phán', 'đẩy_mạnh', 'lực_lượng_vũ_trang', 'hành_động', 'tổng_động_viên']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d14e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords selected using max_sum_sim: ['thành_phố', 'bình_luận', 'thời_điểm', 'đẩy_mạnh', 'lực_lượng_vũ_trang']\n",
      "Keywords selected using mmr: ['tổng_động_viên', '500km2', 'bất_kỳ', 'tên_lửa', 'putin']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphrase\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n",
    "\n",
    "\n",
    "# Use max_sum_sim function to get 5 keywords with the least similarity\n",
    "nr_candidates = 20  # Number of candidates to consider\n",
    "top_n = 5  # Number of final keywords to select\n",
    "selected_keywords_max_sum_sim = max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, nr_candidates)\n",
    "\n",
    "# Use mmr function to get 5 keywords with a balance of relevance and diversity\n",
    "diversity = 0.5  # Diversity parameter\n",
    "selected_keywords_mmr = mmr(doc_embedding, candidate_embeddings, candidates, top_n, diversity)\n",
    "\n",
    "print(\"Keywords selected using max_sum_sim:\", selected_keywords_max_sum_sim)\n",
    "print(\"Keywords selected using mmr:\", selected_keywords_mmr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df993ca1",
   "metadata": {},
   "source": [
    "**1. max_sum_sim function**:\n",
    "\n",
    "- Input: Document embedding (doc_embedding), embeddings of candidate words/phrases (word_embeddings), list of candidate words/phrases (words), the number of keywords to be selected (top_n), and the number of candidates to consider (nr_candidates).\n",
    "- Output: List of top_n keywords with the least similarity to each other.\n",
    "- Purpose: This function selects the keywords in such a way that they have the least similarity among themselves. It calculates the cosine similarity between all candidate words/phrases and chooses a combination of words with the lowest overall similarity. The goal is to ensure that the selected keywords cover different aspects of the document and are not highly redundant.\n",
    "\n",
    "**2. mmr function (Maximal Marginal Relevance)**:\n",
    "\n",
    "- Input: Document embedding (doc_embedding), embeddings of candidate words/phrases (word_embeddings), list of candidate words/phrases (words), the number of keywords to be selected (top_n), and a diversity parameter (diversity).\n",
    "- Output: List of top_n keywords with a balance of relevance and diversity.\n",
    "- Purpose: MMR is a well-known method used for text summarization and keyword extraction. It aims to select keywords that are both relevant to the document and diverse from each other. The function calculates a score based on relevance and diversity for each candidate keyword and iteratively selects the keyword that maximizes the balance between relevance and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7f560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
